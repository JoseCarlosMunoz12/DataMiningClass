\documentclass[12pt,english]{article}
\usepackage[a4paper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}
\usepackage{blindtext}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{lettrine} 
\usepackage{tikz}  
\usepackage{color} 
\usepackage{verbatim}
 \usetikzlibrary{shapes, arrows, calc, arrows.meta, fit, positioning} % these are the parameters passed to the library to create the node graphs  
\tikzset{  
    -Latex,auto,node distance =0.4 cm and 1.0 cm, thick,% node distance is the distance between one node to other, where 1.5cm is the length of the edge between the nodes  
    state/.style ={ellipse, draw, minimum width = 0.3 cm}, % the minimum width is the width of the ellipse, which is the size of the shape of vertex in the node graph  
    point/.style = {circle, draw, inner sep=0.18cm, fill, node contents={}},  
    bidirected/.style={Latex-Latex,dashed}, % it is the edge having two directions  
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily},
    el/.style = {inner sep=2.5pt, align=right, sloped},
    arn_r/.style = {treenode, circle, red, draw=red},
    arn_x/.style = {treenode, rectangle, draw=black},
    minimum width=2.5em, minimum height=1.5em}% arbre rouge noir, nil
\setlength{\parskip}{12pt}
\title{Home Work 3 Undergraduate}
\date{\today}
\author{Jose Carlos Munoz}
%================================
\begin{document}
\newgeometry{left=0.8in,right=0.8in,top=1in,bottom=1in}
\begin{center}
    \Large
    \textbf{Homework 4 UG}\\
    \small
    \today\\
    \large
    Jose Carlos Munoz
\end{center}%===============================
3.10)
To find the cost function of both of the Decision tree we use this formula.
\begin{equation}
F_x(n) = \mbox{Nodes} * \log_{2}m + \mbox{Leafs} * \left [ \log_{2}k \right] + \mbox{Errors} *\log_{2}n \\
\end{equation}
Where m is the number of Attributes, k is the number of classes and n is the unkown Sample size. Which are 16, 3 and n respectivley\\
Now we find the cost function for decision Tree A, where Nodes = 2, Leafs = 3, 7 errors
\begin{equation}\tag{a}\label{eq:a}
\begin{split}
F_a(n) &= 2 * \log_{2}16 + 3 * \left [ \log_{2}3 \right] + 7 *\log_{2} n\\
F_a(n) &= 2 * 4 + 3 * 2 + 7 *\log_{2} n\\
F_a(n) &= 8 + 6 + 7 *\log_{2} n\\
F_a(n) &= 14 + 7 *\log_{2} n\\
\end{split}
\end{equation}
Now we find the cost function for decision Tree b, where Nodes = 2, Leafs = 3, and 4 errors.
\begin{equation}\tag{b}\label{eq:b}
\begin{split}
F_b(n) &= 4 * \log_{2}16 + 5 * \left [ \log_{2}3 \right] + 4 *\log_{2} n\\
F_b(n) &= 4 * 4 + 5 * 2 + 4 *\log_{2} n\\
F_b(n) &= 16 + 10 + 4 *\log_{2} n\\
F_b(n) &= 26 + 4 *\log_{2} n\\
\end{split}
\end{equation}
Using the MDL paradigm we need to find a $L_S(h) + \sqrt{\frac{\log_{ }{\left( \frac{2}{\delta} \right) } + \vert h \vert }{2 m}}$. The better decision tree is the one that gives us the lowest value. m is found as the sample size which is 200, $\delta$ is given as .99. and the $L_S(h)$ is the cost function of the Decision tree that we found.\\
solving for both, when n is 200, Decision tree B is is much lower in value. Therefore the Decision tree is the better graeph because of MDL\\
4.6)a\\
\begin{equation}\tag{1}\label{eq:1}
\begin{split}
P(\mbox{S}\vert \mbox{UG}) &= .15 \\
P(S\vert G) &= .23\\
P(G) &= .2\\
P(UG) &= .8
\end{split}
\end{equation}
These are the known probabilites.\\
 From this we can find $P(\mbox{G}\vert \mbox{S})$.\\
Because of Bayes Theroem  $P(\mbox{G}\vert \mbox{S})$ is the same as the following
\begin{equation}
P(\mbox{G}\vert \mbox{S}) = \frac{P(\mbox{S} \vert \mbox{G}) * P(\mbox{G})}{P(\mbox{S})}
\end{equation}
$P(\mbox{S})$ can be found as
\begin{equation}
\begin{split}
P(\mbox{S}) &=  P(\mbox{S} \vert \mbox{G}) * P(\mbox{G}) + P(\mbox{S} \vert \mbox{UG}) * P(\mbox{UG})\\
P(\mbox{S}) &= .23 *.2 + .15 * .8\\
P(\mbox{S}) &=.166
\end{split}
\end{equation}
Therefore
\begin{equation}
\begin{split}
P(\mbox{G}\vert \mbox{S}) &= \frac{.23 * .2}{.166}\\
P(\mbox{G}\vert \mbox{S}) &= .277\\
\end{split}
\end{equation}
 So the probabilty that a smoker is a graduate student is .277\par
 4.6)c\\
The probability that a smoker is a graduated student  can be written as  $P(\mbox{UG}\vert \mbox{S})$.\\
\begin{equation}
\begin{split}
P(\mbox{UG}\vert \mbox{S}) &=  \frac{P(\mbox{S} \vert \mbox{UG}) * P(\mbox{UG})}{P(\mbox{S})}\\
P(\mbox{UG}\vert \mbox{S}) &=\frac{.23 * .8}{.277}\\
P(\mbox{UG}\vert \mbox{S}) &=.857\\
\end{split}
\end{equation}
 So the probabilty that a smoker is an undergrad is  .857.\\
 Since $P(\mbox{UG}\vert \mbox{S}) > P(\mbox{G}\vert \mbox{S})$ 
 we can conclude we have a higher chance of finding an undergrad that is a smoker\par
 4.6)d\\
\begin{equation}
\begin{split}
P(\mbox{D}\vert \mbox{UG}) &= .1\\
P(\mbox{D}\vert \mbox{G}) &= .3\\
P(\mbox{D}) &= P(\mbox{D}\vert \mbox{UG}) * P(\mbox{UG})+ P(\mbox{D}\vert \mbox{G}) *  P(\mbox{G})\\
P(\mbox{D}) &= 0.1 * .8 + .2 * .3\\
P(\mbox{D}) &= .14\\
P(\mbox{D,S} \vert \mbox{G}) &=P(\mbox{D}\vert \mbox{G}) *P(\mbox{S}\vert \mbox{G})\\
P(\mbox{D,S} \vert \mbox{G}) &= .3 * .23\\
P(\mbox{D,S} \vert \mbox{G}) &= .069\\
P(\mbox{D,S} \vert \mbox{UG}) &=P(\mbox{D}\vert \mbox{UG}) *P(\mbox{S}\vert \mbox{UG})\\
P(\mbox{D,S} \vert \mbox{UG}) &= .1 * .15\\
P(\mbox{D,S} \vert \mbox{UG}) &=0.015\\
P(\mbox{D,S}) &= Q
\end{split}
\end{equation}
These are the known probabilites. Since we don't know what $P(\mbox{D,S})$ is, we set it as a constant Q\\
Now we can find the values for $P(\mbox{G}\vert \mbox{D,S})$ and $P(\mbox{UG}\vert \mbox{D,S})$\\
\begin{equation}
\begin{split}
P(\mbox{UG}\vert \mbox{D,S}) &=  \frac{P(\mbox{D,S} \vert \mbox{UG}) * P(\mbox{UG})}{P(\mbox{D,S})}\\
P(\mbox{UG}\vert \mbox{D,S}) &=\frac{.015 * .8}{Q}\\
P(\mbox{UG}\vert \mbox{D,S}) &=\frac{.012}{Q}\\
P(\mbox{G}\vert \mbox{D,S}) &=  \frac{P(\mbox{D,S} \vert \mbox{UG}) * P(\mbox{UG})}{P(\mbox{D,S})}\\
P(\mbox{G}\vert \mbox{D,S}) &=\frac{.069 *.2}{Q}\\
P(\mbox{G}\vert \mbox{D,S}) &=\frac{.0139}{Q}\\
\end{split}
\end{equation}
From these results we can conclude that the chance that we find a graduate that lives in a dorm and is a smoker is  is higher than the chance that we find an undergraduate that lives in a dorm and is a smoker.\par
4.7)a\\
\begin{equation}
\begin{split}
P(\mbox{A=0} \vert +) &= \frac{2}{5} = .4\\
P(\mbox{A=0} \vert -) &= \frac{3}{5} = .6\\
P(\mbox{A=1} \vert +) &= \frac{3}{5} = .6\\
P(\mbox{A=1} \vert -) &= \frac{2}{5} = .4\\
P(\mbox{B=0} \vert +) &= \frac{4}{5} = .8\\
P(\mbox{B=0} \vert -) &= \frac{3}{5} = .6\\
P(\mbox{B=1} \vert +) &= \frac{1}{5} = .2\\
P(\mbox{B=1} \vert -) &= \frac{2}{5} = .4\\
P(\mbox{C=0} \vert +) &= \frac{3}{5} = .6\\
P(\mbox{C=0} \vert -) &= \frac{0}{5} = 0\\
P(\mbox{C=1} \vert +) &= \frac{2}{5} = .4\\
P(\mbox{C=1} \vert -) &= \frac{5}{5} = .1
\end{split}
\end{equation}
\par
4.7)b\\
we are task to find $P(\mbox{A=0,B=1,C=0} \vert \mbox{+})$. Using the Bayes Therm we canfind the value as
\begin{equation}
\begin{split}
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0,B=1,C=0} \vert \mbox{+}) * P(\mbox{+})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0} \vert \mbox{+}) * P(\mbox{B=1} \vert \mbox{+}) *P(\mbox{C=0} \vert \mbox{+}) * P(\mbox{+})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{.4 * .2 *.6 * .5}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{0.024}{P(\mbox{A=0,B=1,C=0})}\\
\end{split}
\end{equation}
\begin{equation}
\begin{split}
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0,B=1,C=0} \vert \mbox{-}) * P(\mbox{-})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0} \vert \mbox{-}) * P(\mbox{B=1} \vert \mbox{-}) *P(\mbox{C=0} \vert \mbox{-}) * P(\mbox{-})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{.6 * .4 *0 * .5}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= 0\\
\end{split}
\end{equation}
\par
From these results we canconlude that the class label for (A=0, B=1, C=0) will be Class +.\par
4.7)c\\
We will be looking at the conditional probabilites for the them all over again with the m-estimate. When m=4 and p = 1/2;
to find the new Conditonal probabilites we use this equation
\begin{equation}
\frac{ n_c + m * p}{n + m}
\end{equation}
so now the The conditional probablities will be
\begin{equation}
\begin{split}
P(\mbox{A=0} \vert +) &= \frac{2 + 2}{5 + 4} = \frac{4}{9}\\
P(\mbox{A=0} \vert -) &= \frac{3 + 2}{5 + 4} = \frac{5}{9}\\
P(\mbox{A=1} \vert +) &= \frac{3 + 2}{5 + 4} = \frac{5}{9}\\
P(\mbox{A=1} \vert -) &= \frac{2 + 2}{5 + 4} = \frac{4}{9}\\
P(\mbox{B=0} \vert +) &= \frac{4 + 2}{5 + 4} = \frac{6}{9}\\
P(\mbox{B=0} \vert -) &= \frac{3 + 2}{5 + 4} = \frac{5}{9}\\
P(\mbox{B=1} \vert +) &= \frac{1 + 2}{5 + 4} = \frac{3}{9}\\
P(\mbox{B=1} \vert -) &= \frac{2 + 2}{5 + 4} = \frac{4}{9}\\
P(\mbox{C=0} \vert +) &= \frac{3 + 2}{5 + 4} = \frac{5}{9}\\
P(\mbox{C=0} \vert -) &= \frac{0 + 2}{5 + 4} = \frac{2}{9}\\
P(\mbox{C=1} \vert +) &= \frac{2 + 2}{5 + 4} = \frac{4}{9}\\
P(\mbox{C=1} \vert -) &= \frac{5 + 2}{5 + 4} = \frac{7}{9}
\end{split}
\end{equation}
\par
4.7)d\\
we repeat b) but with the m-estimate conditional probabilities
\begin{equation}
\begin{split}
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0,B=1,C=0} \vert \mbox{+}) * P(\mbox{+})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0} \vert \mbox{+}) * P(\mbox{B=1} \vert \mbox{+}) *P(\mbox{C=0} \vert \mbox{+}) * P(\mbox{+})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{\frac{4}{9} * \frac{3}{9} *\frac{5}{9} * .5}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{+} \vert \mbox{A=0,B=1,C=0}) &= \frac{0.0142}{P(\mbox{A=0,B=1,C=0})}\\
\end{split}
\end{equation}
\begin{equation}
\begin{split}
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0,B=1,C=0} \vert \mbox{-}) * P(\mbox{-})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{P(\mbox{A=0} \vert \mbox{-}) * P(\mbox{B=1} \vert \mbox{-}) *P(\mbox{C=0} \vert \mbox{-}) * P(\mbox{-})}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{\frac{5}{9} * \frac{4}{9} *\frac{2}{9} .5}{P(\mbox{A=0,B=1,C=0})}\\
P(\mbox{-} \vert \mbox{A=0,B=1,C=0}) &= \frac{0.0274}{P(\mbox{A=0,B=1,C=0})}\\
\end{split}
\end{equation}
From these resutl we canconlude that the class label for (A=0,B=1,C=0) is class +\\
\par
4.7)e\\
The better method would be the m-estimate becuase we do not want our entire expression to be zero\par
Problem B)\\
A )A benefit of having only 2 dimensions is that it will simpllifiy the computation and it the complexity is reduce. A dissadvantage is that it will be missing out on other complexity and relationships with other attributes. This can greatly decrease the accuracy of the model\\
A benefit of having many dimensions is that the accuracy of it much higher. However,  a prblem is that computation is is slow and the complexity of it is much higher\\
b ) If we have more unlabled samples of A, P, and I, then the first algorithm will be more accurate. However, if the reverse occurs, then the second algorithm will be much more accurate.
\end{document}
