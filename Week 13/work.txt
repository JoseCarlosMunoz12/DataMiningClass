11/30/2012
week 13
Klustering Data points
Means points
a set {} of means that get the nearsest neighboors
then, get new medians to get better  results. 
Continue until the the sets doesn't change
from 2 weeks ago
//K means
Recap
have K centroid, normaly chosen at randomm
then matrix of distance, group those that are closest to each K centroid.
repeat until, there is no change in the centroid
//
we do this becuase no real classes to classify each point kluster
//
might take k in power of points to find the change.
algoorithm depends on the distance, Can use differnt methods to  meacure proximity
manhattan distance, euclidean, cosine similarity, mahalanobis distance,
all of these are the mean, except for the manhattan distance, its the median distance
//
learn what mahalanobis idstnace is
//
Solutions to the initial centroid problems.
what are the centroid to use?, might be too far to move. or over all distance might not change the centroid
//
if cluseter are relativly tight, it migh never change the centroid of the empty cluster.
empty Cluster problem
//
Another problem, the division might  devide clearly seeable cluster. Splits it into a wrong way.
All depends on the initial choice
//
what can we do?
min/max, minisize distancce in cluster, but maximize distance from each cluster
//
we can run many times, but probability is not in our side. k!/2^n possibilities
//
hierachical clustering to determine initial centroids. Might not be good for large data sets
//
Select more than K initial centroids, make more clusters. Select the best cluster. Merge some clusters
//
Bisecting  k-means
//
Modification
Handling Empty Clusters
Choosing K centroid might produce empty clusters. what can we do?
for these use medoids from the points to create a new centroid
-Use a point that contribues the most to the SSE. Continue k-means.
-Choose  apoints from the cluster wiwth the highest SSE. Then choose the highest SSE from that cluster
All of these are hueristic. None work in the worst case scenerio.
on average, it works well enough.
//
Pre-Processing
-normalize the daa
-eliminate outliers
Post-Processing
-Remove small clusters
-Split loose cluster, those with high SSE.Signal that the cluster is actually two
-merge cluster that are close and low SSE, depends on knowledge of the subject anner
-Can use these steps during the clustering process
//
Clusters in R
library is cluster
//
Bisecting K-means
Variatnt of K-menas that can produce a partitional or  ahierachical clustering
randamly generate 2  cluster with the lowset SSE to the list of cluster
Bisect cluster with the hieghst SSE and split it.
Keep doing this until we have an acccaptble Clusuter distance.
//
Splits data into many cluster than needed. Will need to merge some meaning less clusters	
k-Means have a problem in size of clusters and shape of the clusters
In R the cluster class that has bisecting -mean uses dclust library. All it does is divisional bisecting clustering
Unsuperpise learning doesn't give you a guarantee splitting into a local max/min
//
Hierarchical Clustering
like bisecting k-means, but more choices for clustering
//
Not flexible, because it builds bottom up not top down.
They may correspond tomeaningful taxonomies. Ex paleotnolgical classification
two types of it.
Agglomerative and devisive
1)Agglomerative Clustering
-easy to implement.
algorithm
-Compute proximity matrix
-i) merge the two closst cluster
-ii) update the proximity maxtrix
-repeat until only a single cluster remains
